# -*- coding: utf-8 -*-
"""Store_Sales

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OYztvKpBqStGO_-YB9_LZYkr8zpLyj7T

<h1><b>Preparing</b></h1>

<h2> Import Library </h2>
"""

from google.colab import drive
import os
import datetime
import shutil
import zipfile
import re
import pickle
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import timedelta
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import xgboost as xgb
from sklearn.metrics import mean_squared_error

"""<h2> Pull Data </h2>"""

drive.mount('/content/drive')

source_file = "/content/drive/MyDrive/Project Data/kaggle.json"
destination_file = '/content/kaggle.json'
shutil.copy2(source_file, destination_file)
os.environ['KAGGLE_CONFIG_DIR'] = '/content'

! chmod 600 /content/kaggle.json
! kaggle competitions download -c store-sales-time-series-forecasting

# Extract file from zip file
zip_file_path = '/content/store-sales-time-series-forecasting.zip'
extract_dir = '/content/store-sales-time-series-forecasting'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("File successfully extracted!")

"""<h1><b>Dataset Description</b></h1>

**predict sales for the thousands of product families sold at Favorita stores located in Ecuador**. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models.
"""

# Initializing the folder path containing the dataset for time series-based store sales forecasting.
folder_dataset = '/content/store-sales-time-series-forecasting'

# Utilizing list comprehension to gather file names from the given path.
file_names = [f for f in os.listdir(folder_dataset) if os.path.isfile(os.path.join(folder_dataset, f))]

print(file_names)

"""<h2><b>Train Datasets<b><h2>

<ul>
    <li>Training data, comprising time series of features:</li>
    <ul>
        <li><strong>store_nbr:</strong> Identifies the store at which the products are sold.</li>
        <li><strong>family:</strong> Identifies the type of product sold.</li>
        <li><strong>onpromotion:</strong> Gives the total number of items in a product family that were being promoted at a store at a given date.</li>
    </ul>
    <li><strong>Target sales:</strong> Gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (e.g., 1.5 kg of cheese).</li>
</ul>
"""

# Loading the raw training dataset by reading the CSV file located in the specified folder path.
raw_train_dataset = pd.read_csv(os.path.join(folder_dataset,"train.csv"))
raw_train_dataset

# Obtaining information about the structure and attributes of the raw training dataset.
raw_train_dataset.info()

# Printing unique values for specific columns in the raw training dataset.
print("Unique Value\n")

# Printing unique values for the 'store_nbr' column along with the count of unique values.
print(f"store_nbr's Column => jumlah : {len(raw_train_dataset['store_nbr'].unique())}")
print("="*80)
print(raw_train_dataset["store_nbr"].unique())

# Printing unique values for the 'family' column along with the count of unique values.
print(f"\nfamily's Column => jumlah : {len(raw_train_dataset['family'].unique())}")
print("="*80)
print(raw_train_dataset["family"].unique())

# Grouping the raw training dataset based on 'store_nbr' and 'family' columns,
# and calculating the count of occurrences for each combination of store and family.

raw_train_dataset.groupby(['store_nbr', 'family'])["family"].count()

# Checking for missing values in the raw training dataset and printing the sum of missing values for each column.
print(raw_train_dataset.isnull().sum())

"""<h2><b>Test Datasets<b><h2>

<ul>
  <li>The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.</li>
  <li>The dates in the test data are for the 15 days after the last date in the training data.</li>
"""

# Loading the raw test dataset by reading the CSV file located in the specified folder path.
raw_test_dataset = pd.read_csv(os.path.join(folder_dataset,"test.csv"))
raw_test_dataset

# Obtaining information about the structure and attributes of the raw test dataset.
raw_test_dataset.info()

# Printing unique values for specific columns in the raw test dataset.
print("Unique Value\n")

# Printing unique values for the 'store_nbr' column along with the count of unique values.
print(f"store_nbr's Column => jumlah : {len(raw_test_dataset['store_nbr'].unique())}")
print("="*80)
print(raw_test_dataset["store_nbr"].unique())

# Printing unique values for the 'family' column along with the count of unique values.
print(f"\nfamily's Column => jumlah : {len(raw_test_dataset['family'].unique())}")
print("="*80)
print(raw_test_dataset["family"].unique())

# Grouping the raw test dataset by the 'store_nbr' and 'family' columns,
# and calculating the count of occurrences for each combination of store and family.
raw_test_dataset.groupby(['store_nbr', 'family'])["family"].count()

# Checking for missing values in the raw test dataset and printing the sum of missing values for each column.
print(raw_test_dataset.isnull().sum())

"""<h2><b>stores Dataset<b><h2>

<ul>
  <li>Store metadata, including city, state, type, and cluster.</li>
  <li>cluster is a grouping of similar stores.</li>
"""

# Loading and exploring the 'stores' dataset.
# Loading the 'stores' dataset by reading the CSV file located in the specified folder path.
stores = pd.read_csv(os.path.join(folder_dataset,"stores.csv"))
stores.head()

# Obtaining information about the structure and attributes of the 'stores' dataset.
stores.info()

# Printing unique values for specific columns in the 'stores' dataset.
print("Unique Value\n")

# Printing unique values for the 'city' column along with the count of unique values.
print(f"City's Column => jumlah : {len(stores['city'].unique())}")
print("="*80)
print(stores["city"].unique())

# Printing unique values for the 'state' column along with the count of unique values.
print(f"\nState's Column => jumlah : {len(stores['state'].unique())}")
print("="*80)
print(stores["state"].unique())

# Printing unique values for the 'type' column along with the count of unique values.
print("Unique Value\n")
print(f"Type's Column => jumlah : {len(stores['type'].unique())}")
print("="*80)
print(stores["type"].unique())

# Printing unique values for the 'cluster' column along with the count of unique values.
print(f"\nCluster's Column => jumlah : {len(stores['cluster'].unique())}")
print("="*80)
print(stores["cluster"].unique())

"""<h2><b>holidays Events<b><h2>

<ul>
  <li>NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.
</li>
  <li>Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).
</li>
"""

# Loading and exploring the 'holidays_events' dataset.
# Loading the 'holidays_events' dataset by reading the CSV file located in the specified folder path.
holiday = pd.read_csv(os.path.join(folder_dataset,"holidays_events.csv"))
holiday.head()

# Obtaining information about the structure and attributes of the 'holidays_events' dataset.
holiday.info()

# Printing unique values for specific columns in the 'holidays_events' dataset.
print("Unique Value\n")

# Printing unique values for the 'type' column along with the count of unique values.
print(f"Type's Column => jumlah : {len(holiday['type'].unique())}")
print("="*80)
print(holiday['type'].unique())

# Printing unique values for the 'locale' column along with the count of unique values.
print(f"\nLocale's Column => jumlah : {len(holiday['locale'].unique())}")
print("="*80)
print(holiday['locale'].unique())

# Printing unique values for the 'locale_name' column along with the count of unique values.
print(f"\nLocale Name's Column => jumlah : {len(holiday['locale_name'].unique())}")
print("="*80)
print(holiday['locale_name'].unique())

# Printing unique values for the 'description' column along with the count of unique values.
print(f"\nnDescription's Column => jumlah : {len(holiday['description'].unique())}")
print("="*80)
print(holiday['description'].unique())

# Defining a function to process holiday descriptions, filtering and cleaning the text.
def process_description(text, words_to_find, words_to_remove):
    text_lower = text.lower()

    for word in words_to_remove:
        desc = re.sub(r'\b' + re.escape(word) + r'\b', '', text_lower)

    for word in words_to_find:
        match = re.search(word, desc)
        if match:
            return word
    return desc

# List of words to remove and words to find in holiday descriptions
words_to_remove = ['puente ', 'recupero ', 'traslado ']
words_to_find = ['fundacion', 'provincializacion', 'terremoto manabi', 'mundial de futbol brasil', 'fundacion', 'cantonizacion', 'primer dia del ano', 'independencia', 'navidad', 'dia de la madre']

# Filtering out transferred holidays and removing the 'transferred' column
holiday_no_transferred = holiday[holiday['transferred'] != True]
holiday_no_transferred = holiday_no_transferred.drop('transferred', axis=1)

# Creating a cleaned copy of the dataset and applying the process_description function to 'description' column
holiday_clean = holiday_no_transferred.copy()
holiday_clean['description'] = holiday['description'].apply(lambda x: process_description(x, words_to_find, words_to_remove))

# Printing unique values for the cleaned 'description' column
print(f"\nDescription's Column => jumlah : {len(holiday_clean['description'].unique())}")
print("="*80)
print(holiday_clean['description'].unique())

# Displaying the first few rows of the cleaned holiday dataset
holiday_clean.head()

# Converting the 'date' column in the 'oil' dataset to datetime format.
holiday_clean['date'] = pd.to_datetime(holiday_clean['date'])
holiday_clean["date"] = holiday_clean["date"] + timedelta(days=365*6+91)

# Renaming the 'type' column to 'case'
holiday_clean.rename(columns={'type': 'case'}, inplace=True)

holiday_clean.head()

# Dictionary to store label encoders
label_encoders = {}

label_encoder_case = LabelEncoder()

# For local holidays
Events_clean_local = holiday_clean[holiday_clean['locale'] == 'Local'].drop(columns=['locale'])
Events_clean_local.rename(columns={'locale_name': 'city'}, inplace=True)

# For regional holidays
Events_clean_regional = holiday_clean[holiday_clean['locale'] == 'Regional'].drop(columns=['locale'])
Events_clean_regional.rename(columns={'locale_name': 'state'}, inplace=True)

# For national holidays
Events_clean_National = holiday_clean[holiday_clean['locale'] == 'National'].drop(columns=['locale'])
Events_clean_National.drop(columns=['locale_name'], inplace=True)

"""<h2><b>Oil Datasets<b><h2>

<ul>
  <li>Daily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)
"""

# Loading and processing the 'oil' dataset.
# Loading the 'oil' dataset by reading the CSV file located in the specified folder path.
oil = pd.read_csv(os.path.join(folder_dataset,"oil.csv"))
oil.head()

# Obtaining information about the structure and attributes of the 'oil' dataset.
oil.info()

# Converting the 'date' column in the 'oil' dataset to datetime format.
oil['date'] = pd.to_datetime(oil['date'])

# Generating a date range based on the minimum and maximum dates in the 'oil' dataset.
date_range = pd.date_range(start=oil['date'].min(), end=oil['date'].max())

# Creating a DataFrame of missing dates in the 'oil' dataset by comparing with the generated date range.
missing_dates_oil = pd.DataFrame({'date': date_range})
missing_dates_oil = missing_dates_oil[~missing_dates_oil['date'].isin(oil['date'])]

# Creating a new dataset that includes all dates (including missing ones) and sorting by date.
oil_all_date = pd.concat([oil, missing_dates_oil]).sort_values('date').reset_index(drop=True)

oil_all_date

# Imputing missing values in the 'dcoilwtico' column using K-Nearest Neighbors regression.

# Extracting training data (non-null 'dcoilwtico' values) for input and target variables.
x_train = oil_all_date.loc[~oil_all_date['dcoilwtico'].isnull(), 'date'].values.reshape(-1, 1)
y_train = oil_all_date.loc[~oil_all_date['dcoilwtico'].isnull(), 'dcoilwtico']

# Extracting test data (null 'dcoilwtico' values) for input variables.
x_test = oil_all_date.loc[oil_all_date['dcoilwtico'].isnull(), 'date'].values.reshape(-1, 1)

# Setting the number of neighbors for K-Nearest Neighbors.
K = 3

# Creating a K-Nearest Neighbors regressor model and fitting it to the training data.
model = KNeighborsRegressor(n_neighbors=K)
model.fit(x_train, y_train)

# Making predictions for missing 'dcoilwtico' values using the trained model.
predictions = model.predict(x_test)

# Creating a copy of the dataset for imputed values.
oil_imputed = oil_all_date.copy()

# Imputing missing 'dcoilwtico' values with the predicted values.
oil_imputed.loc[oil_imputed['dcoilwtico'].isnull(), 'dcoilwtico'] = predictions
oil_imputed["date"] = pd.to_datetime(oil_imputed["date"])
oil_imputed["date"] = oil_imputed["date"] + timedelta(days=365*6+91)

oil_imputed

"""<h2><b>Merge Datasets<b><h2>"""

# Creating a combined dataset from raw training and test datasets.
# Concatenating raw training and test datasets to create a combined dataset.
raw_combined_dataset = pd.concat([raw_train_dataset, raw_test_dataset], ignore_index=True)
raw_combined_dataset["date"] = pd.to_datetime(raw_combined_dataset["date"])
raw_combined_dataset["date"] = raw_combined_dataset["date"] + timedelta(days=365*6+91)
raw_combined_dataset = raw_combined_dataset.groupby(['store_nbr', 'date'])[['sales', 'onpromotion']].sum().reset_index()
raw_combined_dataset.head()

# Merging combined dataset with store information using 'store_nbr' as the common key.
raw_combined_dataset_stores = raw_combined_dataset.merge(stores, on='store_nbr', how='left')
raw_combined_dataset_stores.head()

# Grouping the merged dataset by 'store_nbr' column.
grouped = raw_combined_dataset_stores.groupby(['store_nbr'])

# Creating a dictionary to store separated data for each store-family combination.
separated_data = {}

# Iterating through the grouped data.
for group_name, group_data in grouped:
    store_number = group_name  # Get the store number from the group name
    key = f"store_{store_number}"
    separated_data[key] = group_data.reset_index(drop=True)

separated_data['store_11']

# Dictionary to store the ready datasets
ready_dataset = {}
case_columns = ["case_National", "case_Local", "case_Regional"]
description_columns = ["description_National", "description_Local", "description_Regional"]

# Looping through each store-family combination's data
for key, value in separated_data.items():
    # Converting 'date' column to datetime format
    value['date'] = pd.to_datetime(value['date'])

    # Merging with imputed oil prices, local holidays, regional holidays, and national holidays
    merged_df_1 = value.merge(oil_imputed, on='date', how='left')
    merged_df_2 = merged_df_1.merge(Events_clean_local, on=['date', 'city'], how='left', suffixes=('', '_Local'))
    merged_df_3 = merged_df_2.merge(Events_clean_regional, on=['date', 'state'], how='left', suffixes=('_Local', '_Regional'))
    merged_df_4 = merged_df_3.merge(Events_clean_National, on='date', how='left', suffixes=('_Regional', '_National'))

    # Dropping irrelevant columns
    merged_df_4 = merged_df_4.drop(columns=["city", "state", "store_nbr", "type", "cluster"])

    # Setting the 'date' column as the index
    merged_df_4 = merged_df_4.set_index('date')
    merged_df_4 = merged_df_4.rename(columns={'case' : 'case_National', 'description' : 'description_National'})

    # Setting sales values to None for the last 150 days (test dataset)
    merged_df_4.loc[merged_df_4.index[-90:], "sales"] = None
    merged_df_4.loc[merged_df_4.index[-90:], "sales"] = None

    # Replace null values with "workday" in 'case' and 'description' columns
    for case_column in case_columns:
        merged_df_4[case_column].fillna("Work Day", inplace=True)
    for descripton_column in description_columns:
        merged_df_4[descripton_column].fillna("Work Day", inplace=True)

    # Storing the processed dataset in the dictionary
    ready_dataset[key] = merged_df_4

# Dictionary to store label encoders
label_encoders = {}

# Create a DataFrame with unique values from the "case" column in the "holiday_clean" dataset
unique_case = pd.DataFrame(holiday_clean["case"].unique(), columns=["case"])

# Initialize a LabelEncoder for the "case" column and fit it to unique values
label_encoder = LabelEncoder()
unique_case["case"] = label_encoder.fit(unique_case["case"])

# Store the label encoder in the dictionary with the key "case"
label_encoders["case"] = label_encoder

# Create a DataFrame with unique values of "description" grouped by "locale" from the "holiday_clean" dataset
unique_description = pd.DataFrame(holiday_clean[["description", "locale"]].groupby("locale")["description"].unique(), columns=["description"])

# Iterate over unique descriptions for each locale
for key, row in unique_description.iterrows():
    # Initialize a LabelEncoder for each set of unique descriptions
    label_encoder = LabelEncoder()
    row["description"] = np.append(row["description"], "Work Day")
    row["description"] = label_encoder.fit(row["description"])
    label_encoders[f"description_{key}"] = label_encoder

label_encoders

encode_columns = ['case_National', 'case_Local', 'case_Regional', 'description_Local', 'description_Regional', 'description_National']

# Loop through the specified columns in the 'encode_columns' list and encode their values using the corresponding label encoders.
# The loop iterates over the 'ready_dataset' dictionary, where each key represents a dataset, and the associated value is a DataFrame.
for key, value in ready_dataset.items():
    for encode_column in encode_columns:
        # Determine the appropriate label encoder to use based on the column prefix ('case' or 'description').
        encoder_key = encode_column if encode_column.split("_")[0] != "case" else "case"
        value[encode_column] = label_encoders[encoder_key].transform(value[encode_column])

pkl_encoders = "label_encoders.pkl"
with open(pkl_encoders, "wb") as file :
  pickle.dump(label_encoders, file)

# Dictionary to store MinMaxScaler instances for each column and key
scalers = {}

# Looping through each store-family combination's ready dataset
for key, value in ready_dataset.items():
    # Initialize MinMaxScaler for this combination
    scaler = MinMaxScaler()

    # Fit the scaler on the dataset's numeric columns
    numeric_columns = value.select_dtypes(include=[np.number]).columns
    scaler.fit(value[numeric_columns])

    # Store the scaler in the dictionary
    scalers[key] = scaler

# transform a given DataFrame into a "windowed" format
def create_windowed_dataframe(df, window_size, stride):
    num_samples = len(df)
    windowed_data = []

    for i in range(0, num_samples - window_size + 1, stride):
        windowed_data.append(df.iloc[i:i+window_size].values)

    columns = [f'sales_{i}' if i != window_size else "y" for i in range(1, window_size + 1)]
    windowed_df = pd.DataFrame(windowed_data, columns=columns)

    return windowed_df

# Dictionary to store transformed datasets
transformed_datasets = {}

# Looping through each store-family combination's ready dataset and scaler
for key, value in ready_dataset.items():
    # Get the corresponding scaler
    scaler = scalers[key]

    # Transforming the numeric columns using the scaler
    numeric_columns = value.select_dtypes(include=[np.number]).columns
    scaled_values = scaler.transform(value[numeric_columns])

    # Creating a DataFrame with scaled values and original index and columns
    scaled_df = pd.DataFrame(scaled_values, columns=numeric_columns, index=value.index)

    # Concatenating scaled numeric columns with non-numeric columns
    transformed_df = pd.concat([scaled_df, value.drop(columns=numeric_columns)], axis=1).dropna().reset_index(drop=True)
    windowed_df = create_windowed_dataframe(transformed_df["sales"], 7, 1).dropna()
    mergings_df = pd.concat([transformed_df, windowed_df], axis=1).dropna()

    # Storing the transformed dataset in the dictionary
    transformed_datasets[key] = mergings_df.drop("sales", axis=1)

# Now the 'transformed_datasets' dictionary contains transformed datasets for each store-family combination.

transformed_datasets["store_10"].round(3).head()

pkl_scaler = "scaler.pkl"
with open(pkl_scaler, "wb") as file :
  pickle.dump(scalers, file)

"""<h2><b>Create Machine Learning Model<b><h2>"""

# Creating datasets for training, testing, and prediction.

# Dictionaries to store the datasets
x_trains = {}
y_trains = {}
x_tests = {}
y_tests = {}
x_predicts = {}

# Looping through each store-family combination's ready dataset
for key, value in transformed_datasets.items():
    # Extracting training data (excluding the 'sales' column)
    train = value[~value["y"].isnull()]

    # Extracting test data (last 150 days of training data)
    test = train.loc[train.index[-150:]]

    # Separating features and target variables
    x_trains[key], y_trains[key] = train.iloc[:, :-1], train.iloc[:, -1].to_frame()
    x_tests[key], y_tests[key] = test.iloc[:, :-1], test.iloc[:, -1].to_frame()

y_tests["store_10"]

fig, ax = plt.subplots(figsize=(10, 3)) # Creating a subplot with a specific figure size
y_trains["store_10"].plot(ax=ax, label='Training Set', title='Data Train/Test Split') # Plotting the training set's sales values
y_tests["store_10"].plot(ax=ax, label='Test Set') # Plotting the test set's sales values
ax.axvline(1455, color='black', ls='--') # Adding a vertical dashed line to indicate the split between training and test data
ax.legend(['Training Set', 'Test Set']) # Adding a legend to the plot
ax.set_xticks([])  # Remove x-axis ticks
plt.show()

# Training XGBoost regressor models for selected store-family combinations.

# Dictionary to store trained regressor models
reg_models = {}

# Looping through each store-family combination's training data
for key, value in x_trains.items():
    # Extracting the store number from the key
    store_number = int(key.split("_")[1])

    # Creating an XGBoost regressor with specified parameters
    reg = xgb.XGBRegressor(n_estimators=1000, early_stopping_rounds=50)

    # Fitting the regressor to the training data and evaluating on training and test sets
    reg.fit(x_trains[key], y_trains[key],
            eval_set=[(x_trains[key], y_trains[key]), (x_tests[key], y_tests[key])],
            verbose=False)

    # Storing the trained regressor in the dictionary
    reg_models[key] = reg

# Dictionary to store RMSE values
rmse_values = {}

# Looping through each store-family combination's trained regressor
for key, reg_model in reg_models.items():
    # Predicting on the test set
    predictions = reg_model.predict(x_tests[key])

    # Calculating RMSE
    rmse = np.sqrt(mean_squared_error(y_tests[key], predictions))

    # Storing RMSE in the dictionary
    rmse_values[key] = rmse

# Displaying the RMSE values
for key, rmse in rmse_values.items():
    print(f"{key} : {rmse:.2f}")

reg_models.keys()

# Creating a bar plot to visualize feature importance for the trained XGBoost regressor.
# Creating a DataFrame to store feature importances
fi = pd.DataFrame(data=reg_models["store_5"].feature_importances_,
                  index=reg.feature_names_in_,
                  columns=['importance'])

# Sorting the DataFrame by importance values and plotting as a horizontal bar plot
fi.sort_values('importance').plot(kind='barh', title='Feature Importance')

plt.show()

# Making predictions on the test data using the trained regressor
test_model = reg_models["store_2"].predict(x_tests["store_2"])
predicted_test = x_tests["store_2"].copy()
predicted_test["predict"] = test_model
predicted_test["actual"] = y_tests["store_2"]

# Reordering columns to move "predict" column to the first position
actual_test = predicted_test[["actual"] + [col for col in predicted_test.columns if col not in ["actual", "predict"] and not any(col.startswith(f"sales_{i}") for i in range(1, 7))]]
predicted_test = predicted_test[["predict"] + [col for col in predicted_test.columns if col not in ["actual", "predict"] and not any(col.startswith(f"sales_{i}") for i in range(1, 7))]]

# Inverse transform the predicted and actual values to get them in original scale
actual_original_scale = scalers["store_2"].inverse_transform(actual_test)
predicted_original_scale = scalers["store_2"].inverse_transform(predicted_test)

# Convert the numpy arrays back to DataFrames
actual_original_df = pd.DataFrame(actual_original_scale, columns=actual_test.columns, index=actual_test.index)
predicted_original_df = pd.DataFrame(predicted_original_scale, columns=predicted_test.columns, index=predicted_test.index)

# Combine the "actual" column with the "predict" column
compare_y = pd.concat([actual_original_df['actual'], predicted_original_df['predict']], axis=1)

# Display the combined values
compare_y

fig, ax = plt.subplots(figsize=(15, 5))
compare_y["actual"].plot(ax=ax, label='Test Set')
compare_y["predict"].plot(ax=ax, label='Test Set')
ax.legend (['Training Set', 'Test Set'])
plt.show()

# Create a folder to store the models
if not os.path.exists('reg_models'):
    os.makedirs('reg_models')

for key, model in reg_models.items():
    model_filename = os.path.join('reg_models', f'reg_{key}.pkl')
    joblib.dump(model, model_filename)

# Name of the zip file
zip_filename = 'reg_models.zip'

# Create a zip file to store the folder
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    # Add all files from the "reg_models" folder to the zip file
    for foldername, subfolders, filenames in os.walk('reg_models'):
        for filename in filenames:
            file_path = os.path.join(foldername, filename)
            arcname = os.path.relpath(file_path, 'reg_models')
            zipf.write(file_path, arcname)

print(f'Zip file "{zip_filename}" created successfully.')

"""## Testing predict"""

data_predict = pd.read_csv("test_predict.csv")

year = 2023
month = 9
start = datetime.datetime(year, month, 1)  # Convert to datetime object
end = datetime.datetime(year, month, 30)   # Convert to datetime object
raw_combined_dataset[(raw_combined_dataset["date"] >= start) & (raw_combined_dataset["date"] <= end)]

# Tentukan ukuran gambar subplot
fig, axes = plt.subplots(9, 6, figsize=(15, 20), sharey=True)
fig.suptitle('Sales Comparison by Store', fontsize=16)

# Loop melalui setiap toko
for store in range(1, 55):
    # Hitung indeks subplot (baris dan kolom)
    row = (store - 1) // 6
    col = (store - 1) % 6

    # Buat subplot untuk toko saat ini
    ax = axes[row, col]

    # Plot data aktual untuk toko saat ini
    raw_combined_dataset[(raw_combined_dataset["date"] >= start) & (raw_combined_dataset["date"] <= end) & (raw_combined_dataset["store_nbr"] == store)]["sales"].reset_index(drop=True).plot(ax=ax, label='actual')

    # Plot data prediksi untuk toko saat ini
    data_predict[data_predict["store_nbr"] == store]["sales"].reset_index(drop=True).iloc[6:].reset_index(drop=True).plot(ax=ax, label='predict')

    # Tambahkan label dan legenda
    ax.set_title(f'Store {store}')
    ax.set_xlabel("Time")
    ax.set_ylabel("Sales")
    ax.legend(loc="upper left")

# Sisakan ruang di sekitar subplot
plt.tight_layout(rect=[0, 0, 1, 0.96])

# Tampilkan plot
plt.show()