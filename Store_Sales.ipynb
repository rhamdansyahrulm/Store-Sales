{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsL5RmWSvBnqds0DXtAeUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhamdansyahrulm/Store-Sales/blob/main/Store_Sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Preparing</b></h1>"
      ],
      "metadata": {
        "id": "-n8WBKrlel2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Import Library </h2>"
      ],
      "metadata": {
        "id": "N-Fl-osEeopk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "2DR0FFNEejWb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Pull Data </h2>"
      ],
      "metadata": {
        "id": "Vrh7rA_7exK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "D941-cuE8mQA",
        "outputId": "0c1f419b-e3d3-4873-9028-632036aeffca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3b8a479202a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file = \"/content/drive/MyDrive/Project Data/kaggle.json\"\n",
        "destination_file = '/content/kaggle.json'\n",
        "shutil.copy2(source_file, destination_file)\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'"
      ],
      "metadata": {
        "id": "52HDYXYeduOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 /content/kaggle.json"
      ],
      "metadata": {
        "id": "cSEOoSoheB-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c store-sales-time-series-forecasting"
      ],
      "metadata": {
        "id": "gcBK_eDMb3uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/store-sales-time-series-forecasting.zip'\n",
        "extract_dir = '/content/store-sales-time-series-forecasting'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"File successfully extracted!\")"
      ],
      "metadata": {
        "id": "F5VP6uyJeTD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Dataset Description</b></h1>\n",
        "\n",
        "**predict sales for the thousands of product families sold at Favorita stores located in Ecuador**. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models."
      ],
      "metadata": {
        "id": "QlBEarMvZave"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_dataset = '/content/store-sales-time-series-forecasting'\n",
        "file_names = [f for f in os.listdir(folder_dataset) if os.path.isfile(os.path.join(folder_dataset, f))]\n",
        "\n",
        "print(file_names)"
      ],
      "metadata": {
        "id": "RopGIr1kZcTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Train Datasets<b><h2>\n",
        "\n",
        "<ul>\n",
        "    <li>Training data, comprising time series of features:</li>\n",
        "    <ul>\n",
        "        <li><strong>store_nbr:</strong> Identifies the store at which the products are sold.</li>\n",
        "        <li><strong>family:</strong> Identifies the type of product sold.</li>\n",
        "        <li><strong>onpromotion:</strong> Gives the total number of items in a product family that were being promoted at a store at a given date.</li>\n",
        "    </ul>\n",
        "    <li><strong>Target sales:</strong> Gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (e.g., 1.5 kg of cheese).</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "LFNsHyzFkm0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset = pd.read_csv(os.path.join(folder_dataset,\"train.csv\"))\n",
        "raw_train_dataset"
      ],
      "metadata": {
        "id": "YJrLUNdmZodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset.info()"
      ],
      "metadata": {
        "id": "rEkYRcH6k4gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Value\\n\")\n",
        "print(f\"store_nbr's Column => jumlah : {len(raw_train_dataset['store_nbr'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(raw_train_dataset[\"store_nbr\"].unique())\n",
        "\n",
        "print(f\"\\nfamily's Column => jumlah : {len(raw_train_dataset['family'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(raw_train_dataset[\"family\"].unique())"
      ],
      "metadata": {
        "id": "bHrtMKpIlbA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset.groupby(['store_nbr', 'family'])[\"family\"].count()"
      ],
      "metadata": {
        "id": "BBj6e7ye4Cbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_train_dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "w7DsxgyFzzbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Test Datasets<b><h2>\n",
        "\n",
        "<ul>\n",
        "  <li>The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.</li>\n",
        "  <li>The dates in the test data are for the 15 days after the last date in the training data.</li>"
      ],
      "metadata": {
        "id": "_gK3wn4FNBct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_dataset = pd.read_csv(os.path.join(folder_dataset,\"test.csv\"))\n",
        "raw_test_dataset"
      ],
      "metadata": {
        "id": "nLOGxWh9NBct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_dataset.info()"
      ],
      "metadata": {
        "id": "hToFmRBYNBct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Value\\n\")\n",
        "print(f\"store_nbr's Column => jumlah : {len(raw_test_dataset['store_nbr'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(raw_test_dataset[\"store_nbr\"].unique())\n",
        "\n",
        "print(f\"\\nfamily's Column => jumlah : {len(raw_test_dataset['family'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(raw_test_dataset[\"family\"].unique())"
      ],
      "metadata": {
        "id": "qPyosMWZNBcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_dataset.groupby(['store_nbr', 'family'])[\"family\"].count()"
      ],
      "metadata": {
        "id": "csN0KlTDNBcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_test_dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "G14lCrxDNBcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>stores Dataset<b><h2>\n",
        "\n",
        "<ul>\n",
        "  <li>Store metadata, including city, state, type, and cluster.</li>\n",
        "  <li>cluster is a grouping of similar stores.</li>"
      ],
      "metadata": {
        "id": "KH5MG2v6D3Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores = pd.read_csv(os.path.join(folder_dataset,\"stores.csv\"))\n",
        "stores.head()"
      ],
      "metadata": {
        "id": "xNT6RFT5EGlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores.info()"
      ],
      "metadata": {
        "id": "SQBvTgWkEM8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Value\\n\")\n",
        "print(f\"City's Column => jumlah : {len(stores['city'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(stores[\"city\"].unique())\n",
        "\n",
        "print(f\"\\nState's Column => jumlah : {len(stores['state'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(stores[\"state\"].unique())\n",
        "\n",
        "print(\"Unique Value\\n\")\n",
        "print(f\"Type's Column => jumlah : {len(stores['type'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(stores[\"type\"].unique())\n",
        "\n",
        "print(f\"\\nCluster's Column => jumlah : {len(stores['cluster'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(stores[\"cluster\"].unique())"
      ],
      "metadata": {
        "id": "Eln9MR1SESib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Train-Test Datasets<b><h2>"
      ],
      "metadata": {
        "id": "Td9f7DlDNV9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_combined_dataset = pd.concat([raw_train_dataset, raw_test_dataset], ignore_index=True)\n",
        "raw_combined_dataset"
      ],
      "metadata": {
        "id": "9O0dQH8pNae2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_combined_dataset_stores = raw_combined_dataset.merge(stores, on='store_nbr', how='left')\n",
        "raw_combined_dataset_stores"
      ],
      "metadata": {
        "id": "L59udaWcdo31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memisahkan DataFrame berdasarkan kolom \"store_nbr\" dan \"family\"\n",
        "grouped = raw_combined_dataset_stores.groupby(['store_nbr', 'family'])\n",
        "\n",
        "# Membuat dictionary untuk menyimpan tabel-tabel yang terpisah\n",
        "separated_data = {}\n",
        "\n",
        "# Menyimpan tabel-tabel terpisah dalam dictionary\n",
        "for group, group_data in grouped:\n",
        "    key = f\"{group[0]}_{group[1]}\"\n",
        "    separated_data[key] = group_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FoG4R_SNQeV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>holidays Events<b><h2>\n",
        "\n",
        "<ul>\n",
        "  <li>NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n",
        "</li>\n",
        "  <li>Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n",
        "</li>"
      ],
      "metadata": {
        "id": "JT3KeTMWHK4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holiday = pd.read_csv(os.path.join(folder_dataset,\"holidays_events.csv\"))\n",
        "holiday.head()"
      ],
      "metadata": {
        "id": "g6lrmEDUHiGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holiday.info()"
      ],
      "metadata": {
        "id": "mS9JIqyZRnzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Value\\n\")\n",
        "print(f\"Type's Column => jumlah : {len(holiday['type'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(holiday['type'].unique())\n",
        "\n",
        "print(f\"\\nLocale's Column => jumlah : {len(holiday['locale'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(holiday['locale'].unique())\n",
        "\n",
        "print(f\"\\nLocale Name's Column => jumlah : {len(holiday['locale_name'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(holiday['locale_name'].unique())\n",
        "\n",
        "print(f\"\\nnDescription's Column => jumlah : {len(holiday['description'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(holiday['description'].unique())"
      ],
      "metadata": {
        "id": "SMd8l_QZIU96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghapus karakter setelah kata tertentu dari string atau mengembalikan kata tersebut\n",
        "def process_description(text, words_to_find, words_to_remove):\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for word in words_to_remove:\n",
        "        desc = re.sub(r'\\b' + re.escape(word) + r'\\b', '', text_lower)\n",
        "\n",
        "    for word in words_to_find:\n",
        "        match = re.search(word, desc)\n",
        "        if match:\n",
        "            return word\n",
        "    return desc\n",
        "\n",
        "# Kata-kata tempat pemotongan dilakukan\n",
        "words_to_remove = ['puente', 'recupero', 'traslado']\n",
        "words_to_find = ['fundacion', 'provincializacion', 'terremoto manabi', 'mundial de futbol brasil', 'fundacion', 'cantonizacion', 'primer dia del ano', 'independencia', 'navidad', 'dia de la madre']\n",
        "\n",
        "# Mengubah kolom 'description' sesuai dengan proses yang dijelaskan dalam fungsi\n",
        "holiday_no_transferred = holiday[holiday['transferred'] != True]\n",
        "holiday_no_transferred = holiday_no_transferred.drop('transferred', axis=1)\n",
        "holiday_clean = holiday_no_transferred.copy()\n",
        "holiday_clean['description'] = holiday['description'].apply(lambda x: process_description(x, words_to_find, words_to_remove))\n",
        "\n",
        "print(f\"\\nnDescription's Column => jumlah : {len(holiday_clean['description'].unique())}\")\n",
        "print(\"=\"*80)\n",
        "print(holiday_clean['description'].unique())\n",
        "\n",
        "holiday_clean.head()"
      ],
      "metadata": {
        "id": "QlRV6fnkl5VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mengubah nama kolom \"type\" menjadi \"case\"\n",
        "holiday_clean['date'] = pd.to_datetime(holiday_clean['date'])\n",
        "holiday_clean.rename(columns={'type': 'case'}, inplace=True)\n",
        "\n",
        "# 2) Membuat DataFrame baru dengan filter kolom \"locale\" == \"Local\", \"Regional\", dan \"National\"\n",
        "holiday_clean_local = holiday_clean[holiday_clean['locale'] == 'Local'].drop(columns=['locale'])\n",
        "holiday_clean_regional = holiday_clean[holiday_clean['locale'] == 'Regional'].drop(columns=['locale'])\n",
        "holiday_clean_national = holiday_clean[holiday_clean['locale'] == 'National'].drop(columns=['locale'])\n",
        "\n",
        "# 3) Pada DataFrame filter \"Local\", mengubah kolom \"locale_name\" menjadi \"city\"\n",
        "holiday_clean_local.rename(columns={'locale_name': 'city'}, inplace=True)\n",
        "# Pada DataFrame filter \"Regional\", mengubah kolom \"locale_name\" menjadi \"state\"\n",
        "holiday_clean_regional.rename(columns={'locale_name': 'state'}, inplace=True)\n",
        "# Pada DataFrame filter \"National\", langsung drop kolom \"locale_name\"\n",
        "holiday_clean_national.drop(columns=['locale_name'], inplace=True)"
      ],
      "metadata": {
        "id": "CAhtt5XGkyzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cetak DataFrame hasil pemrosesan\n",
        "print(\"Local:\")\n",
        "print(holiday_clean_local.head(2))\n",
        "print()\n",
        "\n",
        "print(\"Regional:\")\n",
        "print(holiday_clean_regional.head(2))\n",
        "print()\n",
        "\n",
        "print(\"National:\")\n",
        "print(holiday_clean_national.head(2))"
      ],
      "metadata": {
        "id": "ifPM3aWEldGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Oil Datasets<b><h2>\n",
        "\n",
        "<ul>\n",
        "  <li>Daily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)"
      ],
      "metadata": {
        "id": "W2nV1XnTt1hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oil = pd.read_csv(os.path.join(folder_dataset,\"oil.csv\"))\n",
        "oil"
      ],
      "metadata": {
        "id": "dZftuKNQuXjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil.info()"
      ],
      "metadata": {
        "id": "9Gvnx5-lycVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubah kolom 'date' menjadi tipe data datetime\n",
        "oil['date'] = pd.to_datetime(oil['date'])\n",
        "\n",
        "# Buat rentang tanggal yang diinginkan\n",
        "date_range = pd.date_range(start=oil['date'].min(), end=oil['date'].max())\n",
        "\n",
        "# Gabungkan rentang tanggal dengan DataFrame yang ada\n",
        "missing_dates_oil = pd.DataFrame({'date': date_range})\n",
        "missing_dates_oil = missing_dates_oil[~missing_dates_oil['date'].isin(oil['date'])]\n",
        "oil_all_date = pd.concat([oil, missing_dates_oil]).sort_values('date').reset_index(drop=True)\n",
        "\n",
        "oil_all_date"
      ],
      "metadata": {
        "id": "7Ne4pKqJwrtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the features for training and testing\n",
        "x_train = oil_all_date.loc[~oil_all_date['dcoilwtico'].isnull(), 'date'].values.reshape(-1, 1)\n",
        "y_train = oil_all_date.loc[~oil_all_date['dcoilwtico'].isnull(), 'dcoilwtico']\n",
        "x_test = oil_all_date.loc[oil_all_date['dcoilwtico'].isnull(), 'date'].values.reshape(-1, 1)\n",
        "\n",
        "K = 3\n",
        "model = KNeighborsRegressor(n_neighbors=3)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Now you can use the model for prediction\n",
        "predictions = model.predict(x_test)\n",
        "oil_imputed = oil_all_date.copy()\n",
        "oil_imputed.loc[oil_imputed['dcoilwtico'].isnull(), 'dcoilwtico'] = predictions\n",
        "\n",
        "oil_imputed"
      ],
      "metadata": {
        "id": "ziD7SGdm55u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Merge Datasets<b><h2>"
      ],
      "metadata": {
        "id": "jS9YgV55JhZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ready_dataset = {}\n",
        "encode_columns = ['case_city', 'description_city', 'case_regional', 'description_regional', 'case', 'description']\n",
        "\n",
        "# Dictionary to store label encoders\n",
        "label_encoders = {}\n",
        "\n",
        "for key, value in separated_data.items():\n",
        "    value['date'] = pd.to_datetime(value['date'])\n",
        "    merged_df_1 = value.merge(oil_imputed, on='date', how='left')\n",
        "    merged_df_2 = merged_df_1.merge(holiday_clean_local, on=['date', 'city'], how='left', suffixes=('', '_city'))\n",
        "    merged_df_3 = merged_df_2.merge(holiday_clean_regional, on=['date', 'state'], how='left', suffixes=('_city', '_regional'))\n",
        "    merged_df_4 = merged_df_3.merge(holiday_clean_national, on='date', how='left', suffixes=('_regional', ''))\n",
        "    merged_df_4 = merged_df_4.drop(columns=[\"city\", \"state\", \"store_nbr\", \"family\", \"type\", \"cluster\"])\n",
        "    for encode_column in encode_columns:\n",
        "        label_encoder = LabelEncoder()\n",
        "        merged_df_4[encode_column] = label_encoder.fit_transform(merged_df_4[encode_column])\n",
        "        # Store the label encoder in the dictionary\n",
        "        label_encoders[encode_column] = label_encoder\n",
        "    merged_df_4 = merged_df_4.set_index('date')\n",
        "    merged_df_4.loc[merged_df_4.index[-150:], \"sales\"] = None\n",
        "    ready_dataset[key] = merged_df_4"
      ],
      "metadata": {
        "id": "FQhvb4VAcFER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoders"
      ],
      "metadata": {
        "id": "4fdOQ5N13u8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoders['case'].classes_)\n",
        "print()\n",
        "print(label_encoders['description'].classes_)"
      ],
      "metadata": {
        "id": "YnpgU3tW5o9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Create Machine Learning Model<b><h2>"
      ],
      "metadata": {
        "id": "HEwuDmQ0xbHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_trains = {}\n",
        "y_trains = {}\n",
        "x_tests = {}\n",
        "y_tests = {}\n",
        "x_predicts = {}\n",
        "\n",
        "for key, value in ready_dataset.items():\n",
        "  train = value[~value[\"sales\"].isnull()]\n",
        "  test = train.loc[train.index[-150:]]\n",
        "  predict = value[value[\"sales\"].isnull()]\n",
        "\n",
        "  x_trains[key], y_trains[key] = train.iloc[:,2:], train.iloc[:,1].to_frame()\n",
        "  x_tests[key], y_tests[key] = test.iloc[:,2:], test.iloc[:,1].to_frame()\n",
        "  x_predicts[key] = train.iloc[:,2:]"
      ],
      "metadata": {
        "id": "pmX4-0nlxgaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "y_trains[\"20_CLEANING\"].plot(ax=ax, label='Training Set', title='Data Train/Test Split')\n",
        "y_tests[\"20_CLEANING\"].plot(ax=ax, label='Test Set')\n",
        "ax.axvline('11-04-2016', color='black', ls='--')\n",
        "ax.legend (['Training Set', 'Test Set'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r0aDWaVCR2EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"10_Rhamdan\"\n",
        "int(a.split(\"_\")[0])"
      ],
      "metadata": {
        "id": "sSTHABBoSCN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models = {}\n",
        "\n",
        "for key, value in x_trains.items():\n",
        "    while int(key.split(\"_\")[0]) <= 5:\n",
        "        reg = xgb.XGBRegressor(n_estimators=100, early_stopping_rounds=50)\n",
        "        reg.fit(x_trains[key], y_trains[key],\n",
        "                eval_set=[(x_trains[key], y_trains[key]), (x_tests[key], y_tests[key])],\n",
        "                verbose=False)\n",
        "\n",
        "        # Store the trained regressor in the dictionary\n",
        "        reg_models[key] = reg"
      ],
      "metadata": {
        "id": "cwuHLpAfU2Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_models.keys()"
      ],
      "metadata": {
        "id": "wqx1LFx0Tlys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fi = pd.DataFrame (data=reg.feature_importances_,\n",
        "              index=reg.feature_names_in_,\n",
        "              columns=['importance'])\n",
        "fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MgxB3ZQJY1do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = reg.predict(x_tests[\"20_CLEANING\"])\n",
        "compare_y = y_tests[\"20_CLEANING\"].copy()\n",
        "compare_y[\"predict\"] = test_model\n",
        "compare_y"
      ],
      "metadata": {
        "id": "vvu4-RXYZOFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "compare_y[\"sales\"].plot(ax=ax, label='Test Set')\n",
        "compare_y[\"predict\"].plot(ax=ax, label='Test Set')\n",
        "ax.axvline('03-19-2017', color='black', ls='--')\n",
        "ax.legend (['Training Set', 'Test Set'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JyspK6laZ8nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github"
      ],
      "metadata": {
        "id": "rs_DiEvQYTa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"rhamdansyahrulm@gmail.com\"\n",
        "!git config --global user.name \"rhamdansyahrulm\""
      ],
      "metadata": {
        "id": "3huOlbzyYVg7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rhamdansyahrulm/Store-Sales.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrCJUx1mYtIX",
        "outputId": "7499c497-cd7d-4658-8f2e-b739e72a43d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Store-Sales'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add store_sales.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHDN-o4wZFZt",
        "outputId": "4046590c-d0cb-4da1-d401-871082962df8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}